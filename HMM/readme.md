# 隐含马尔可夫模型（HMM）
在自然语言处理中分词是一个很基础且很重要的概念。分词的方法主要有：基于规则分词和基于统计分词2种。目前比较常用的是基于统计分词技术。
基于统计分词一般有以下2个步骤
- 建立统计语言模型
- 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。
这里用到的统计学习算法有隐含马尔可夫（HMM）和条件随机场（CRF）
这个笔记主要是针对HMM做的总结，至于CRF我会在另外的笔记做总结。

在自然语言概念里词通常是有单个子组成的，也就是说字和字连接组成词是有一定的概率的，这些概率是可以通过词库（语料库）训练得到的。这个训练过程也就是计算概率的过程。

隐含马尔可夫模型（HMM）是将分词作为字在字串内中的序列标注任务来完成的。基本思路是，每个字在构造一个特定词语的时候是占据一个确定的构词位置（即词位）。现规定每个字最多有四个构词位置：
- 词首：B
- 词中：M
- 词尾：E
- 单独成词：S
这四个位置其实就构成4个隐含状态。
这样我们就可以用数学抽象出理想模型：
用$$w=w_{1}w_{2}...w_{n}$$代表输入的句子，n为句子长度，$$w_{i}$$表示字
$$o=o_{1}o_{2}...o_{n}$$代表输出的标签，那么数学表示为：
$$max=maxP(o_{1}o_{2}...o_{n}|w_{1}w_{2}...w_{n})$$
这里为了简化条件概率计算，引入观测独立性假设，即每个字的输出仅仅与当前字有关。
这样条件概率就可以简化为：$$P(o_{1}o_{2}...o_{n}|w_{1}w_{2}...w_{n})=P(o_{1}|w_{1})P(o_{2}|w_{2})...P(o_{n}|w_{n})$$

贝叶斯公式：$$P(o|w)=\frac{P(o,w)}{P(w)}=\frac{P(w|o)P(o)}{P(w)}$$
针对$$P(w|o)P(o)$$做马尔可夫假设可得：$$P(w|o)=P(w_{1}|o_{1})P(w_{2}|o_{2})...P(w_{n}|o_{n})$$
同时有全概率公式：$$P(o)=P(o_{1})P(o_{2}|o_{1})P(o_{3}|o_{1},o_{2})...P(o_{n}|o_{1},o_{2},...,o_{n-1})$$

这里为了简化计算，我们再次做一个假设：齐次马尔可夫假设，也就是说每个输出仅于上一个输出有关，
那么全概率公式就简化为：$$P(o)=P(o_{1})P(o_{2}|o_{1})P(o_{3}|o_{2})...P(o_{n}|o_{n-1})$$
这样一来我们就可以有以下的近似：
$$P(w|o)P(o)〜P(w_{1}|o_{1})P(o_{2}|o_{1})P(w_{2}|o_{2})P(o_{3}|o_{2})...P(w_{n}|o_{n})P(o_{n}|o_{n-1})$$

另外我们在HMM中将$P(w_{k}|o_{k})$称为发射概率，也就是隐含标签与字之间条件概率，
$P(o_{k}|o_{k-1})$称为转移概率，也就是各个隐含标签之间的条件概率。这些概率都可以通过事先准备的
语料库来训练（统计分析）得到。

我查阅了一些书籍和网络博客，整理了一个例子，这个例子同样适用于日文。

