## 关键词提取
关键词提取在自然语言处理中也占有很重要的地位，并且很贴近日常生活，特别是在信息爆炸的当下。目前比较受欢迎的是无监督算法，因为不需要维护词表。无监督算法主要由以下几种：
- TF-IDF
- TextRank
- 主题模型算法（LSA，LSI，LDA）
  
### TF-IDF算法
TF(Term Frequency)算法是统计一个词在一篇文档中出现的次数，基本思想是，一个词在文档中出现的次数越多，则其对文档的特性表示也就越强。用数学符号表示：
$$
tf_{ij}=\frac{n_{ij}}{\sum_{k}n_{kj}}
$$
通俗来说就是tf(word)=(word在文档中出现的次数)/(文档总词数)

IDF(Inverse Document Frequency)算法是统计一个词在文档集中出现在几个文档中，基本思想是，如果一个词在越少的文档中出现，则其对文档的区分能力就越强。用数学符号表示：
$idf_{i}=\log(\frac{|D|}{1+|D_{i}|})$
分母加1是为了避免新词在语料库中不存在是分母出现0的情况。这种处理方式叫做拉普拉斯平滑

TF仅衡量词的出现次数，但没有考虑到词对文档的区分能力。相反IDF强调的是词的区分能力，所以一般是将2者结合起来使用。但是如何结合比较好？据学者总结如下效果比较好：
$$
tf\times{idf(i,j)}=tf_{ij}\times{idf_{i}}=\frac{n_{ij}}{\sum_{k}n_{kj}}\times\log(\frac{|D|}{1+|D_{i}|})
$$

通常情况下我们训练一个关键词提取算法需要以下几个步骤
- 加载已有的文档数据集
- 加载停用词表
- 对数据集中的文档进行分词
- 根据停用词表，过滤干扰词
- 根据数据集训练算法
- 对新文档进行分词
- 根据停用词表，过滤干扰词
- 根据训练好的模型提取关键词

### TextRank算法
TF-IDF算法是统计每个词在语料库中有多少个文档出现过。这样会带来一个问题，如果我没有事先已知的语料库文档集，
那我们该怎么办？TextRank算法就是为了解决这个问题。TextRank算法最早用于文档的自动摘要，基于句子维度的分析，
利用TextRank对每个句子进行打分，挑选出分数最高的n个句子作为文档的关键句子，以达到自动摘要的效果。
TextRank算法的基本思想来源于Google的PageRank算法，PageRank算法是一种网页排名算法，基本思想：
- 连接数量：一个网页被越多的其他网页链接，说明这个网页越重要
- 链接质量：一个网页被一个月高权重的网页链接，也能表明这个网页越重要

$In(V_{i})$ 表示 
$V_{i}$ 的入链集合，
$Out(V{_j})$ 为 $V_{j}$ 的出链是集合，
|$Out(V{_j})$| 为出链的数量。
一些孤立网页（没有出链与入链）的得分会为0，这样的话，这些页面就会永远无法被检索到。另外为了避免这样的问题，引入一个阻尼系数。
数学表达式：
$$
S(V_{i}) = (1-d)+d\times\sum_{V_j\in In(V_{i})}\left(\frac{w_{ij}}{\sum_{V_k\in Out(V_i)}w{_{jk}}}\times WS(V_j)\right)
$$

当TextRank应用到关键词提取是，与自动摘要中主要有以下2点不同
- 词与词之间的关联性没有权重
- 每个词不是与文档中所有词都有链接

对于第一点我们可以将得分平均共享给每个链接词
$$S(V_{i}) = (1-d)+d\times\sum_{V_j\in In(V_{i})}\left(\frac{1}{|Out(V_{j})|}\times WS(V_j)\right)$$

对于第二点，学者们提出一个窗口的概念。在窗口中的词相互间都有链接关系。
比如一短文字分词，去除干扰词之后得到
$[n1，n2，n3，n4，n5，n6，..., n{_n}]$
窗口大小如果是3，则可以得到如下窗口
$[n1，n2，n3]$
$[n2，n3，n4]$
$[n3，n4，n5]$
......

在jieba框架里已经实现了textrank接口，可以很方便的用。
至于如何自己去实现此算法，等有时间再研究补充。
在例子程序的输出结果来了，其实textrank和TF-IDF不相上下。

### 主题模型算法 LDA
一般来说，TF-IDF和TextRank算法就能满足大部分关键词提取的任务。但是在某些场景，基于文档本身的关键词提取还是不充分的，有些关键词并不会出现在文档中。比如一篇介绍狮子，老虎，大象等动物的文章，但是文章中并没有显式的出现【动物】二字，这种情况下，TF-IDF和TextRank算法是无法提取出【动物】这个隐含的主题信息，这个时候我们就需要用到主题模型。
主题模型认为词和文档之间没有直接的联系，他们还有一个维度将他们关联起来，主题模型将这个维度称为主题。每个文档都应该对应一个或者多个主题，而每个主题都会有对应的词分布。根据这个原理我们可以得到以下核心数学公式
$$
p(w_{i}|d_{j}) = \sum_{k=1}^Kp(w_{i}|t_{k})\times p(t_{k}|d_{j})
$$

在一个已知的数据集中，每个词和文档对应的$p(w_{i}|d_{j})$都是已知的。
主题模型就是根据这个已知信息，通过计算$p(w_{i}|t_{k})$和$p(t_{k}|d_{j})$的值，从而得到主题的词分布和文档的主题分布。想要得到这个分布信息，现在常用的方法有LSA（LSI）和LDA。其中LSA主要采用SVD（奇异之分解）的方法进行暴力破解，而LDA则是通过贝叶斯学派的方法对分布信息进行拟合。由于LSA（LSI）存在SVD计算复杂度高，特征空间维度大，计算效率低等问题，目前已经很少被采用了。我也就不深入理解其中的原理和如何通过代码实现了。

LDA根据词的共现信息分析，拟合出词-文档-主题的分布，进而将词，文本都映射到一个语义空间中。具体的LDA模型应当如何进行求解，其中一种主流的方法是吉布斯采样。结合吉布斯采样的LDA模型训练过程一般有如下几步
- 随机初始化，对语料中每篇文档中的每个词w，随机的赋予一个topic编号z。
- 重新扫描语料库，对每个词w按照吉布斯采样公式重新采样它的topic，在语料库中进行更新
- 重复以上语料库的重新采样过程，直到吉布斯采样收敛
- 统计语料库的topic-word共现频率矩阵，该矩阵就是LDA的模型。
  
训练好模型之后我们就可以通过以下的步骤按照一定的方式针对新文档的topic进行预估。
- 随机初始化，对当前文档中的每个词w，随机的赋予一个topic编号z
- 重新扫描当前文档，按照吉布斯采样公式，重新采样它的topic。
- 重复以上过程直到吉布斯采样收敛
- 统计文档中的topic分布即为预估结果

LDA算法有许多地方需要注意，比如怎么确定共轭分布中的超参数，怎么通过狄利克雷分布和多项式分布得到他们的共轭分布，具体怎么实现吉布斯采样
