## 关键词提取
关键词提取在自然语言处理中也占有很重要的地位，并且很贴近日常生活，特别是在信息爆炸的当下。目前比较受欢迎的是无监督算法，因为不需要维护词表。无监督算法主要由以下几种：
- TF-IDF
- TextRank
- 主题模型算法（LSA，LSI，LDA）
  
### TF-IDF算法
TF(Term Frequency)算法是统计一个词在一篇文档中出现的次数，基本思想是，一个词在文档中出现的次数越多，则其对文档的特性表示也就越强。用数学符号表示：
$$
tf_{ij}=\frac{n_{ij}}{\sum_{k}n_{kj}}
$$
通俗来说就是tf(word)=(word在文档中出现的次数)/(文档总词数)

IDF(Inverse Document Frequency)算法是统计一个词在文档集中出现在几个文档中，基本思想是，如果一个词在越少的文档中出现，则其对文档的区分能力就越强。用数学符号表示：
$idf_{i}=\log(\frac{|D|}{1+|D_{i}|})$
分母加1是为了避免新词在语料库中不存在是分母出现0的情况。这种处理方式叫做拉普拉斯平滑

TF仅衡量词的出现次数，但没有考虑到词对文档的区分能力。相反IDF强调的是词的区分能力，所以一般是将2者结合起来使用。但是如何结合比较好？据学者总结如下效果比较好：
$$
tf\times{idf(i,j)}=tf_{ij}\times{idf_{i}}=\frac{n_{ij}}{\sum_{k}n_{kj}}\times\log(\frac{|D|}{1+|D_{i}|})
$$

通常情况下我们训练一个关键词提取算法需要以下几个步骤
- 加载已有的文档数据集
- 加载停用词表
- 对数据集中的文档进行分词
- 根据停用词表，过滤干扰词
- 根据数据集训练算法
- 对新文档进行分词
- 根据停用词表，过滤干扰词
- 根据训练好的模型提取关键词

### TextRank算法
TF-IDF算法是统计每个词在语料库中有多少个文档出现过。这样会带来一个问题，如果我没有事先已知的语料库文档集，
那我们该怎么办？TextRank算法就是为了解决这个问题。TextRank算法最早用于文档的自动摘要，基于句子维度的分析，
利用TextRank对每个句子进行打分，挑选出分数最高的n个句子作为文档的关键句子，以达到自动摘要的效果。
TextRank算法的基本思想来源于Google的PageRank算法，PageRank算法是一种网页排名算法，基本思想：
- 连接数量：一个网页被越多的其他网页链接，说明这个网页越重要
- 链接质量：一个网页被一个月高权重的网页链接，也能表明这个网页越重要
$In(V_{i})$ 表示 $V_{i}$ 的入链集合，$Out(V{_j})$ 为 $V_{j}$ 的出链是集合，|$Out(V{_j})$| 为出链的数量。
一些孤立网页（没有出链与入链）的得分会为0，这样的话，这些页面就会永远无法被检索到。另外为了避免这样的问题，引入一个阻尼系数。
数学表达式：
$$S(V_{i}) = (1-d)+d\times\sum_{V_j\in In(V_{i})}\left(\frac{w_{ij}}{\sum_{V_k\in Out(V_i)}w{_{jk}}}\times WS(V_j)\right)$$

当TextRank应用到关键词提取是，与自动摘要中主要有以下2点不同
- 词与词之间的关联性没有权重
- 每个词不是与文档中所有词都有链接

对于第一点我们可以将得分平均共享给每个链接词
$$S(V_{i}) = (1-d)+d\times\sum_{V_j\in In(V_{i})}\left(\frac{1}{|Out(V_{j})|}\times WS(V_j)\right)$$

对于第二点，学者们提出一个窗口的概念。在窗口中的词相互间都有链接关系。
比如一短文字分词，去除干扰词之后得到
$[n1，n2，n3，n4，n5，n6，..., n{_n}]$
窗口大小如果是3，则可以得到如下窗口
$[n1，n2，n3]$
$[n2，n3，n4]$
$[n3，n4，n5]$
......

在jieba框架里已经实现了textrank接口，可以很方便的用。
至于如何自己去实现此算法，等有时间再研究补充。
在例子程序的输出结果来了，其实textrank和TF-IDF不相上下。
